{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gagicha/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('train.csv')\n",
    "test_df=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5   5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6   6    13    14                                Should I buy tiago?   \n",
       "7   7    15    16                     How can I be a good geologist?   \n",
       "8   8    17    18                    When do you use シ instead of し?   \n",
       "9   9    19    20  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6  What keeps childern active and far from phone ...             0  \n",
       "7          What should I do to be a great geologist?             1  \n",
       "8              When do you use \"&\" instead of \"and\"?             0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?             0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing steps, convert text into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating vocabulary and inverse vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab is a dict, with keys as the words and the values being the index of those words in the emedding matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab={}\n",
    "inverse_vocab=['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen='my name is apple'\n",
    "for word in text_to_word_list(sen):\n",
    "    if word not in vocab:\n",
    "        vocab[word]=len(inverse_vocab)\n",
    "        inverse_vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 4, 'is': 3, 'my': 1, 'name': 2}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK>', 'my', 'name', 'is', 'apple']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "# MODEL_SAVING_DIR = '/home/ecohen/HDD/HDD4/Models/Kaggle/Quora/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec= KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google word2vec model has 3M words in vocab with word vect size 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this=word2vec.word_vec('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating vocab and inverse_vocab and sustituting thhe questions to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "# vocab={}\n",
    "# inverse_vocab=['<UNK>']\n",
    "\n",
    "# questions=['question1', 'question2']\n",
    "# for data in [train_df, test_df]:\n",
    "#     for index, row in data.iterrows():\n",
    "#         for ques in questions:\n",
    "#             q2n=[] #question to number representation\n",
    "#             for word in text_to_word_list(row[ques]):\n",
    "#                 if word in stops and word not in word2vec.vocab:\n",
    "#                     continue # continue gives the control to the top of the loop and ignores the remaining statements.\n",
    "#                 if word not in vocab :\n",
    "#                     vocab[word]= len(inverse_vocab)\n",
    "#                     q2n.append(len(inverse_vocab))\n",
    "#                     inverse_vocab.append(word)\n",
    "#                 else:\n",
    "#                     q2n.append(vocab[word])\n",
    "                    \n",
    "#             data.set_value(index, ques, q2n)\n",
    "\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [train_df, test_df]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  #question to number representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "                if word in stops or word not in word2vec.vocab:\n",
    "                    continue # continue gives the control to the top of the loop and ignores the remaining statements.\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions with number representation\n",
    "            dataset.set_value(index, question, q2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>[1, 1, 2, 3, 4, 5]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[7, 8, 9, 10]</td>\n",
       "      <td>[11, 12, 13, 14, 15, 8, 9, 10, 16]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>[17, 18, 19, 20, 21, 22]</td>\n",
       "      <td>[19, 18, 23, 24, 25]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>[26, 27, 28]</td>\n",
       "      <td>[29, 30, 31, 32, 31, 33]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[34, 35, 36, 37, 38, 39, 40, 41, 42]</td>\n",
       "      <td>[43, 11, 44, 38, 36]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                             question1  \\\n",
       "0   0     1     2                 [1, 1, 2, 3, 4, 5, 6]   \n",
       "1   1     3     4                         [7, 8, 9, 10]   \n",
       "2   2     5     6              [17, 18, 19, 20, 21, 22]   \n",
       "3   3     7     8                          [26, 27, 28]   \n",
       "4   4     9    10  [34, 35, 36, 37, 38, 39, 40, 41, 42]   \n",
       "\n",
       "                            question2  is_duplicate  \n",
       "0                  [1, 1, 2, 3, 4, 5]             0  \n",
       "1  [11, 12, 13, 14, 15, 8, 9, 10, 16]             0  \n",
       "2                [19, 18, 23, 24, 25]             0  \n",
       "3            [29, 30, 31, 32, 31, 33]             0  \n",
       "4                [43, 11, 44, 38, 36]             0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('train_vectors.csv', index= None, header=True)\n",
    "test_df.to_csv('test_vectors.csv', index= None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1440, 1047, 609, 102, 4347, 1047]</td>\n",
       "      <td>[3418, 252, 1362, 10888, 1362, 5463, 1191, 144...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[784, 9406, 687, 155, 11, 431]</td>\n",
       "      <td>[155, 431, 784, 9406, 1626]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[139, 186, 917, 187, 671, 101]</td>\n",
       "      <td>[917, 187, 671]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[672, 23753]</td>\n",
       "      <td>[300]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[1032, 1017]</td>\n",
       "      <td>[1032, 1017]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                           question1  \\\n",
       "0        0  [1440, 1047, 609, 102, 4347, 1047]   \n",
       "1        1      [784, 9406, 687, 155, 11, 431]   \n",
       "2        2      [139, 186, 917, 187, 671, 101]   \n",
       "3        3                        [672, 23753]   \n",
       "4        4                        [1032, 1017]   \n",
       "\n",
       "                                           question2  \n",
       "0  [3418, 252, 1362, 10888, 1362, 5463, 1191, 144...  \n",
       "1                        [155, 431, 784, 9406, 1626]  \n",
       "2                                    [917, 187, 671]  \n",
       "3                                              [300]  \n",
       "4                                       [1032, 1017]  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58563"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 1,\n",
       " 'guide': 2,\n",
       " 'invest': 3,\n",
       " 'share': 4,\n",
       " 'market': 5,\n",
       " 'india': 6,\n",
       " 'story': 7,\n",
       " 'koh': 8,\n",
       " 'noor': 9,\n",
       " 'diamond': 10,\n",
       " 'would': 11,\n",
       " 'happen': 12,\n",
       " 'indian': 13,\n",
       " 'government': 14,\n",
       " 'stole': 15,\n",
       " 'back': 16,\n",
       " 'increase': 17,\n",
       " 'speed': 18,\n",
       " 'internet': 19,\n",
       " 'connection': 20,\n",
       " 'using': 21,\n",
       " 'vpn': 22,\n",
       " 'increased': 23,\n",
       " 'hacking': 24,\n",
       " 'dns': 25,\n",
       " 'mentally': 26,\n",
       " 'lonely': 27,\n",
       " 'solve': 28,\n",
       " 'find': 29,\n",
       " 'remainder': 30,\n",
       " 'math': 31,\n",
       " '^': 32,\n",
       " 'divided': 33,\n",
       " 'one': 34,\n",
       " 'dissolve': 35,\n",
       " 'water': 36,\n",
       " 'sugar': 37,\n",
       " 'salt': 38,\n",
       " 'methane': 39,\n",
       " 'carbon': 40,\n",
       " 'di': 41,\n",
       " 'oxide': 42,\n",
       " 'fish': 43,\n",
       " 'survive': 44,\n",
       " 'astrology': 45,\n",
       " 'sun': 46,\n",
       " 'cap': 47,\n",
       " 'moon': 48,\n",
       " 'rising': 49,\n",
       " 'say': 50,\n",
       " 'triple': 51,\n",
       " 'ascendant': 52,\n",
       " 'buy': 53,\n",
       " 'keeps': 54,\n",
       " 'childern': 55,\n",
       " 'active': 56,\n",
       " 'far': 57,\n",
       " 'phone': 58,\n",
       " 'video': 59,\n",
       " 'games': 60,\n",
       " 'good': 61,\n",
       " 'geologist': 62,\n",
       " 'great': 63,\n",
       " 'use': 64,\n",
       " 'instead': 65,\n",
       " 'motorola': 66,\n",
       " 'company': 67,\n",
       " 'hack': 68,\n",
       " 'charter': 69,\n",
       " 'free': 70,\n",
       " 'method': 71,\n",
       " 'separation': 72,\n",
       " 'slits': 73,\n",
       " 'fresnel': 74,\n",
       " 'things': 75,\n",
       " 'technicians': 76,\n",
       " 'tell': 77,\n",
       " 'durability': 78,\n",
       " 'reliability': 79,\n",
       " 'laptops': 80,\n",
       " 'components': 81,\n",
       " 'read': 82,\n",
       " 'youtube': 83,\n",
       " 'comments': 84,\n",
       " 'see': 85,\n",
       " 'make': 86,\n",
       " 'physics': 87,\n",
       " 'easy': 88,\n",
       " 'learn': 89,\n",
       " 'first': 90,\n",
       " 'sexual': 91,\n",
       " 'experience': 92,\n",
       " 'like': 93,\n",
       " 'laws': 94,\n",
       " 'change': 95,\n",
       " 'status': 96,\n",
       " 'student': 97,\n",
       " 'visa': 98,\n",
       " 'green': 99,\n",
       " 'card': 100,\n",
       " 'us': 101,\n",
       " 'compare': 102,\n",
       " 'immigration': 103,\n",
       " 'canada': 104,\n",
       " 'japan': 105,\n",
       " 'trump': 106,\n",
       " 'presidency': 107,\n",
       " 'mean': 108,\n",
       " 'current': 109,\n",
       " 'international': 110,\n",
       " 'master': 111,\n",
       " 'students': 112,\n",
       " 'f1': 113,\n",
       " 'affect': 114,\n",
       " 'presently': 115,\n",
       " 'planning': 116,\n",
       " 'study': 117,\n",
       " 'manipulation': 118,\n",
       " 'means': 119,\n",
       " 'girls': 120,\n",
       " 'want': 121,\n",
       " 'friends': 122,\n",
       " 'guy': 123,\n",
       " 'reject': 124,\n",
       " 'guys': 125,\n",
       " 'feel': 126,\n",
       " 'rejecting': 127,\n",
       " 'girl': 128,\n",
       " 'many': 129,\n",
       " 'users': 130,\n",
       " 'posting': 131,\n",
       " 'questions': 132,\n",
       " 'readily': 133,\n",
       " 'answered': 134,\n",
       " 'google': 135,\n",
       " 'people': 136,\n",
       " 'ask': 137,\n",
       " 'easily': 138,\n",
       " 'best': 139,\n",
       " 'digital': 140,\n",
       " 'marketing': 141,\n",
       " 'institution': 142,\n",
       " 'institute': 143,\n",
       " 'pune': 144,\n",
       " 'rockets': 145,\n",
       " 'look': 146,\n",
       " 'white': 147,\n",
       " 'boosters': 148,\n",
       " 'painted': 149,\n",
       " 'causing': 150,\n",
       " 'someone': 151,\n",
       " 'jealous': 152,\n",
       " 'avoid': 153,\n",
       " 'question': 154,\n",
       " 'much': 155,\n",
       " 'kv': 156,\n",
       " 'hp': 157,\n",
       " 'conversion': 158,\n",
       " 'chart': 159,\n",
       " 'cc': 160,\n",
       " 'horsepower': 161,\n",
       " 'every': 162,\n",
       " 'time': 163,\n",
       " 'clock': 164,\n",
       " 'numbers': 165,\n",
       " 'times': 166,\n",
       " 'day': 167,\n",
       " 'hands': 168,\n",
       " 'overlap': 169,\n",
       " 'tips': 170,\n",
       " 'making': 171,\n",
       " 'job': 172,\n",
       " 'interview': 173,\n",
       " 'process': 174,\n",
       " 'medicines': 175,\n",
       " 'foundation': 176,\n",
       " 'medicine': 177,\n",
       " 'web': 178,\n",
       " 'application': 179,\n",
       " 'framework': 180,\n",
       " 'society': 181,\n",
       " 'place': 182,\n",
       " 'importance': 183,\n",
       " 'sports': 184,\n",
       " 'contribute': 185,\n",
       " 'way': 186,\n",
       " 'money': 187,\n",
       " 'online': 188,\n",
       " 'prepare': 189,\n",
       " 'ca': 190,\n",
       " 'final': 191,\n",
       " 'law': 192,\n",
       " 'know': 193,\n",
       " 'completely': 194,\n",
       " 'exam': 195,\n",
       " 'thing': 196,\n",
       " 'better': 197,\n",
       " 'despite': 198,\n",
       " 'knowing': 199,\n",
       " 'special': 200,\n",
       " 'cares': 201,\n",
       " 'nose': 202,\n",
       " 'gets': 203,\n",
       " 'stuffy': 204,\n",
       " 'night': 205,\n",
       " 'keep': 206,\n",
       " 'getting': 207,\n",
       " 'game': 208,\n",
       " 'thrones': 209,\n",
       " 'villain': 210,\n",
       " 'likely': 211,\n",
       " 'give': 212,\n",
       " 'mercy': 213,\n",
       " 'united': 214,\n",
       " 'states': 215,\n",
       " 'still': 216,\n",
       " 'blacklist': 217,\n",
       " 'employment': 218,\n",
       " 'etc': 219,\n",
       " 'citizens': 220,\n",
       " 'political': 221,\n",
       " 'views': 222,\n",
       " 'average': 223,\n",
       " 'gas': 224,\n",
       " 'molecules': 225,\n",
       " 'determined': 226,\n",
       " 'travel': 227,\n",
       " 'website': 228,\n",
       " 'spain': 229,\n",
       " 'think': 230,\n",
       " 'obama': 231,\n",
       " 'try': 232,\n",
       " 'take': 233,\n",
       " 'guns': 234,\n",
       " 'away': 235,\n",
       " 'gun': 236,\n",
       " 'control': 237,\n",
       " 'initiative': 238,\n",
       " 'already': 239,\n",
       " 'year': 240,\n",
       " 'old': 241,\n",
       " 'improve': 242,\n",
       " 'skills': 243,\n",
       " 'become': 244,\n",
       " 'entrepreneur': 245,\n",
       " 'next': 246,\n",
       " 'years': 247,\n",
       " 'billionaire': 248,\n",
       " 'girlfriend': 249,\n",
       " 'asks': 250,\n",
       " 'boyfriend': 251,\n",
       " 'choose': 252,\n",
       " 'makes': 253,\n",
       " 'reply': 254,\n",
       " 'said': 255,\n",
       " 'end': 256,\n",
       " 'confused': 257,\n",
       " 'feelings': 258,\n",
       " 'wished': 259,\n",
       " 'well': 260,\n",
       " 'disconnected': 261,\n",
       " 'call': 262,\n",
       " 'wants': 263,\n",
       " 'get': 264,\n",
       " 'together': 265,\n",
       " 'civil': 266,\n",
       " 'service': 267,\n",
       " 'stall': 268,\n",
       " 'f': 269,\n",
       " 'wings': 270,\n",
       " 'fully': 271,\n",
       " 'swept': 272,\n",
       " 'aircraft': 273,\n",
       " 'stop': 274,\n",
       " 'variable': 275,\n",
       " 'sweep': 276,\n",
       " 'squat': 277,\n",
       " 'squats': 278,\n",
       " 'legs': 279,\n",
       " 'thicker': 280,\n",
       " 'expect': 281,\n",
       " 'cognizant': 282,\n",
       " 'confirmation': 283,\n",
       " 'mail': 284,\n",
       " 'month': 285,\n",
       " 'trading': 286,\n",
       " 'kid': 287,\n",
       " 'rebel': 288,\n",
       " 'worth': 289,\n",
       " 'long': 290,\n",
       " 'run': 291,\n",
       " 'bored': 292,\n",
       " 'universities': 293,\n",
       " 'recruit': 294,\n",
       " 'new': 295,\n",
       " 'grads': 296,\n",
       " 'majors': 297,\n",
       " 'looking': 298,\n",
       " 'bg': 299,\n",
       " 'foods': 300,\n",
       " 'quickest': 301,\n",
       " 'followers': 302,\n",
       " 'number': 303,\n",
       " 'darth': 304,\n",
       " 'vader': 305,\n",
       " 'fought': 306,\n",
       " 'maul': 307,\n",
       " 'star': 308,\n",
       " 'wars': 309,\n",
       " 'legends': 310,\n",
       " 'character': 311,\n",
       " 'limit': 312,\n",
       " 'profile': 313,\n",
       " 'descriptions': 314,\n",
       " 'stages': 315,\n",
       " 'breaking': 316,\n",
       " 'couple': 317,\n",
       " 'happens': 318,\n",
       " 'emotionally': 319,\n",
       " 'whether': 320,\n",
       " 'male': 321,\n",
       " 'female': 322,\n",
       " 'affected': 323,\n",
       " 'breakup': 324,\n",
       " 'boy': 325,\n",
       " 'examples': 326,\n",
       " 'products': 327,\n",
       " 'crude': 328,\n",
       " 'oil': 329,\n",
       " 'made': 330,\n",
       " 'career': 331,\n",
       " 'launcher': 332,\n",
       " 'rbi': 333,\n",
       " 'grade': 334,\n",
       " 'b': 335,\n",
       " 'preparation': 336,\n",
       " 'program': 337,\n",
       " 'blu': 338,\n",
       " 'ray': 339,\n",
       " 'play': 340,\n",
       " 'regular': 341,\n",
       " 'dvd': 342,\n",
       " 'player': 343,\n",
       " 'nd': 344,\n",
       " 'always': 345,\n",
       " 'sad': 346,\n",
       " 'aerodynamically': 347,\n",
       " 'propellor': 348,\n",
       " 'rotates': 349,\n",
       " 'memorable': 350,\n",
       " 'ever': 351,\n",
       " 'eaten': 352,\n",
       " 'delicious': 353,\n",
       " 'dish': 354,\n",
       " 'gst': 355,\n",
       " 'affects': 356,\n",
       " 'cas': 357,\n",
       " 'tax': 358,\n",
       " 'officers': 359,\n",
       " 'cannot': 360,\n",
       " 'homework': 361,\n",
       " 'difficult': 362,\n",
       " 'apply': 363,\n",
       " 'programs': 364,\n",
       " 'senior': 365,\n",
       " 'friend': 366,\n",
       " 'lying': 367,\n",
       " 'true': 368,\n",
       " 'secretly': 369,\n",
       " 'attracted': 370,\n",
       " 'rap': 371,\n",
       " 'songs': 372,\n",
       " 'dance': 373,\n",
       " 'suddenly': 374,\n",
       " 'logged': 375,\n",
       " 'gmail': 376,\n",
       " 'remember': 377,\n",
       " 'password': 378,\n",
       " 'realized': 379,\n",
       " 'recovery': 380,\n",
       " 'email': 381,\n",
       " 'longer': 382,\n",
       " 'alive': 383,\n",
       " 'recover': 384,\n",
       " 'ways': 385,\n",
       " 'french': 386,\n",
       " 'genders': 387,\n",
       " 'download': 388,\n",
       " 'content': 389,\n",
       " 'kickass': 390,\n",
       " 'torrent': 391,\n",
       " 'without': 392,\n",
       " 'registration': 393,\n",
       " 'torrents': 394,\n",
       " 'trustworthy': 395,\n",
       " 'normal': 396,\n",
       " 'dark': 397,\n",
       " 'ring': 398,\n",
       " 'around': 399,\n",
       " 'iris': 400,\n",
       " 'eye': 401,\n",
       " 'causes': 402,\n",
       " 'treated': 403,\n",
       " 'harry': 404,\n",
       " 'potter': 405,\n",
       " 'book': 406,\n",
       " 'cursed': 407,\n",
       " 'child': 408,\n",
       " 'bad': 409,\n",
       " 'jk': 410,\n",
       " 'depressed': 411,\n",
       " 'evening': 412,\n",
       " 'european': 413,\n",
       " 'family': 414,\n",
       " 'office': 415,\n",
       " 'database': 416,\n",
       " 'american': 417,\n",
       " 'java': 418,\n",
       " 'programming': 419,\n",
       " 'language': 420,\n",
       " 'computer': 421,\n",
       " 'important': 422,\n",
       " 'store': 423,\n",
       " 'energy': 424,\n",
       " 'produced': 425,\n",
       " 'lightning': 426,\n",
       " 'possible': 427,\n",
       " 'review': 428,\n",
       " 'performance': 429,\n",
       " 'testing': 430,\n",
       " 'cost': 431,\n",
       " 'privacy': 432,\n",
       " 'germany': 433,\n",
       " 'come': 434,\n",
       " 'else': 435,\n",
       " 'lost': 436,\n",
       " 'gain': 437,\n",
       " 'genuinely': 438,\n",
       " 'enjoy': 439,\n",
       " 'salad': 440,\n",
       " 'dressing': 441,\n",
       " 'types': 442,\n",
       " 'immunity': 443,\n",
       " 'different': 444,\n",
       " 'body': 445,\n",
       " 'narcissistic': 446,\n",
       " 'personality': 447,\n",
       " 'disorder': 448,\n",
       " 'speak': 449,\n",
       " 'english': 450,\n",
       " 'fluently': 451,\n",
       " 'helpful': 452,\n",
       " 'quickbooks': 453,\n",
       " 'auto': 454,\n",
       " 'data': 455,\n",
       " 'support': 456,\n",
       " 'corrupted': 457,\n",
       " 'files': 458,\n",
       " 'customer': 459,\n",
       " 'usa': 460,\n",
       " 'richest': 461,\n",
       " 'gambler': 462,\n",
       " 'reach': 463,\n",
       " 'level': 464,\n",
       " 'fire': 465,\n",
       " 'bullet': 466,\n",
       " 'backward': 467,\n",
       " 'going': 468,\n",
       " 'faster': 469,\n",
       " 'backwards': 470,\n",
       " 'bullets': 471,\n",
       " 'sound': 472,\n",
       " 'shot': 473,\n",
       " 'devastation': 474,\n",
       " 'occurs': 475,\n",
       " 'prevent': 476,\n",
       " 'breast': 477,\n",
       " 'cancer': 478,\n",
       " 'preventable': 479,\n",
       " 'log': 480,\n",
       " 'account': 481,\n",
       " 'telling': 482,\n",
       " 'ip': 483,\n",
       " 'address': 484,\n",
       " 'device': 485,\n",
       " 'name': 486,\n",
       " 'excluding': 487,\n",
       " 'selling': 488,\n",
       " 'purpose': 489,\n",
       " 'life': 490,\n",
       " 'actually': 491,\n",
       " 'bjp': 492,\n",
       " 'strip': 493,\n",
       " 'muslims': 494,\n",
       " 'christians': 495,\n",
       " 'citizenship': 496,\n",
       " 'put': 497,\n",
       " 'boats': 498,\n",
       " 'burma': 499,\n",
       " 'model': 500,\n",
       " 'deport': 501,\n",
       " 'illegal': 502,\n",
       " 'right': 503,\n",
       " 'etiquette': 504,\n",
       " 'wishing': 505,\n",
       " 'witness': 506,\n",
       " 'happy': 507,\n",
       " 'birthday': 508,\n",
       " 'person': 509,\n",
       " 'wish': 510,\n",
       " 'open': 511,\n",
       " 'commercial': 512,\n",
       " 'fm': 513,\n",
       " 'radio': 514,\n",
       " 'station': 515,\n",
       " 'city': 516,\n",
       " 'procedure': 517,\n",
       " 'clip': 518,\n",
       " 'hd': 519,\n",
       " 'zealand': 520,\n",
       " 'swiss': 521,\n",
       " 'despise': 522,\n",
       " 'asians': 523,\n",
       " 'technical': 524,\n",
       " 'employees': 525,\n",
       " 'sales': 526,\n",
       " 'high': 527,\n",
       " 'salary': 528,\n",
       " 'income': 529,\n",
       " 'jobs': 530,\n",
       " 'field': 531,\n",
       " 'biotechnology': 532,\n",
       " 'paying': 533,\n",
       " 'fresher': 534,\n",
       " 'tech': 535,\n",
       " 'height': 536,\n",
       " 'also': 537,\n",
       " 'major': 538,\n",
       " 'effects': 539,\n",
       " 'earthquake': 540,\n",
       " 'earthquakes': 541,\n",
       " 'difference': 542,\n",
       " 'sincerity': 543,\n",
       " 'fairness': 544,\n",
       " 'honest': 545,\n",
       " 'sincere': 546,\n",
       " 'gaming': 547,\n",
       " 'laptop': 548,\n",
       " 'inr': 549,\n",
       " 'rs': 550,\n",
       " 'warrior': 551,\n",
       " 'proving': 552,\n",
       " 'grounds': 553,\n",
       " 'part': 554,\n",
       " '9': 555,\n",
       " '5': 556,\n",
       " 'reference': 557,\n",
       " 'class': 558,\n",
       " 'chemistry': 559,\n",
       " 'cbse': 560,\n",
       " 'board': 561,\n",
       " 'national': 562,\n",
       " 'technology': 563,\n",
       " 'social': 564,\n",
       " 'karnataka': 565,\n",
       " 'graduating': 566,\n",
       " 'batch': 567,\n",
       " 'lessons': 568,\n",
       " 'juniors': 569,\n",
       " 'leave': 570,\n",
       " 'romantic': 571,\n",
       " 'movies': 572,\n",
       " 'movie': 573,\n",
       " 'seen': 574,\n",
       " 'nightmare': 575,\n",
       " 'nightmares': 576,\n",
       " 'seem': 577,\n",
       " 'real': 578,\n",
       " 'abstract': 579,\n",
       " 'expressionism': 580,\n",
       " 'painting': 581,\n",
       " 'influences': 582,\n",
       " '3d': 583,\n",
       " 'printing': 584,\n",
       " 'work': 585,\n",
       " 'attend': 586,\n",
       " 'jeremy': 587,\n",
       " 'notable': 588,\n",
       " 'folks': 589,\n",
       " 'attended': 590,\n",
       " 'horcrux': 591,\n",
       " 'associate': 592,\n",
       " 'product': 593,\n",
       " 'manager': 594,\n",
       " 'apm': 595,\n",
       " 'early': 596,\n",
       " 'join': 597,\n",
       " 'management': 598,\n",
       " 'rewarding': 599,\n",
       " 'general': 600,\n",
       " 'requirement': 601,\n",
       " 'based': 602,\n",
       " 'software': 603,\n",
       " 'skype': 604,\n",
       " '1': 605,\n",
       " 'busy': 606,\n",
       " 'could': 607,\n",
       " 'android': 608,\n",
       " '4': 609,\n",
       " 'really': 610,\n",
       " 'war': 611,\n",
       " 'pakistan': 612,\n",
       " 'uri': 613,\n",
       " 'attack': 614,\n",
       " 'nuclear': 615,\n",
       " 'ronald': 616,\n",
       " 'reagan': 617,\n",
       " 'mannerism': 618,\n",
       " 'speech': 619,\n",
       " 'react': 620,\n",
       " 'strategies': 621,\n",
       " 'union': 622,\n",
       " 'confederates': 623,\n",
       " 'possibly': 624,\n",
       " 'defeated': 625,\n",
       " 'forces': 626,\n",
       " 'fiction': 627,\n",
       " 'novel': 628,\n",
       " 'novels': 629,\n",
       " 'forgot': 630,\n",
       " 'recent': 631,\n",
       " 'results': 632,\n",
       " 'higher': 633,\n",
       " 'gdp': 634,\n",
       " 'short': 635,\n",
       " 'heard': 636,\n",
       " 'hacked': 637,\n",
       " 'love': 638,\n",
       " 'pity': 639,\n",
       " 'competitive': 640,\n",
       " 'hiring': 641,\n",
       " 'republic': 642,\n",
       " 'bank': 643,\n",
       " 'helps': 644,\n",
       " 'spam': 645,\n",
       " 'ranking': 646,\n",
       " 'adjustment': 647,\n",
       " 'search': 648,\n",
       " 'distribution': 649,\n",
       " 'traffic': 650,\n",
       " 'organic': 651,\n",
       " 'eg': 652,\n",
       " 'vs': 653,\n",
       " '2': 654,\n",
       " 'rankings': 655,\n",
       " 'page': 656,\n",
       " 'second': 657,\n",
       " 'watch': 658,\n",
       " 'subtitles': 659,\n",
       " 'powerful': 660,\n",
       " 'country': 661,\n",
       " 'world': 662,\n",
       " 'obtain': 663,\n",
       " 'instant': 664,\n",
       " 'ulcer': 665,\n",
       " 'pain': 666,\n",
       " 'relief': 667,\n",
       " 'low': 668,\n",
       " 'heat': 669,\n",
       " 'ice': 670,\n",
       " 'china': 671,\n",
       " 'food': 672,\n",
       " 'chinese': 673,\n",
       " 'taking': 674,\n",
       " 'advantage': 675,\n",
       " 'worse': 676,\n",
       " 'materialistic': 677,\n",
       " 'cry': 678,\n",
       " 'stick': 679,\n",
       " 'tongues': 680,\n",
       " 'pictures': 681,\n",
       " 'teen': 682,\n",
       " 'women': 683,\n",
       " 'allow': 684,\n",
       " 'boyfriends': 685,\n",
       " 'naked': 686,\n",
       " 'age': 687,\n",
       " 'ending': 688,\n",
       " 'depressing': 689,\n",
       " 'winston': 690,\n",
       " 'julia': 691,\n",
       " 'differ': 692,\n",
       " 'similar': 693,\n",
       " 'mind': 694,\n",
       " 'blowing': 695,\n",
       " 'tools': 696,\n",
       " 'exist': 697,\n",
       " 'technologies': 698,\n",
       " 'toothbrush': 699,\n",
       " 'wet': 700,\n",
       " 'dry': 701,\n",
       " 'applying': 702,\n",
       " 'toothpaste': 703,\n",
       " 'cheapest': 704,\n",
       " 'marked': 705,\n",
       " 'needing': 706,\n",
       " 'need': 707,\n",
       " 'neutral': 708,\n",
       " 'state': 709,\n",
       " 'buffer': 710,\n",
       " 'declared': 711,\n",
       " 'win': 712,\n",
       " 'mineral': 713,\n",
       " 'holds': 714,\n",
       " 'highest': 715,\n",
       " 'electrical': 716,\n",
       " 'charge': 717,\n",
       " 'hold': 718,\n",
       " 'greatest': 719,\n",
       " 'mystery': 720,\n",
       " 'universe': 721,\n",
       " 'alternative': 722,\n",
       " 'machine': 723,\n",
       " 'learning': 724,\n",
       " 'sample': 725,\n",
       " 'multi': 726,\n",
       " 'imbalance': 727,\n",
       " 'set': 728,\n",
       " 'block': 729,\n",
       " 'sanctions': 730,\n",
       " 'un': 731,\n",
       " 'e': 732,\n",
       " 'mohammad': 733,\n",
       " 'jem': 734,\n",
       " 'chief': 735,\n",
       " 'future': 736,\n",
       " 'budget': 737,\n",
       " 'meaning': 738,\n",
       " 'excessive': 739,\n",
       " 'amounts': 740,\n",
       " 'vitamin': 741,\n",
       " 'c': 742,\n",
       " 'cause': 743,\n",
       " 'miscarriage': 744,\n",
       " 'glass': 745,\n",
       " 'pay': 746,\n",
       " 'scale': 747,\n",
       " 'two': 748,\n",
       " 'letter': 749,\n",
       " 'intent': 750,\n",
       " 'recruitment': 751,\n",
       " 'access': 752,\n",
       " 'com': 753,\n",
       " 'mantras': 754,\n",
       " 'yantra': 755,\n",
       " 'mantra': 756,\n",
       " 'six': 757,\n",
       " 'party': 758,\n",
       " 'talks': 759,\n",
       " 'successful': 760,\n",
       " 'course': 761,\n",
       " 'register': 762,\n",
       " 'domain': 763,\n",
       " 'site': 764,\n",
       " 'older': 765,\n",
       " 'men': 766,\n",
       " 'young': 767,\n",
       " 'strongest': 768,\n",
       " 'structure': 769,\n",
       " 'shape': 770,\n",
       " 'compression': 771,\n",
       " 'kevlar': 772,\n",
       " 'cord': 773,\n",
       " 'matter': 774,\n",
       " 'humans': 775,\n",
       " 'selfish': 776,\n",
       " 'evil': 777,\n",
       " 'humanity': 778,\n",
       " 'exocytosis': 779,\n",
       " 'endocytosis': 780,\n",
       " 'passive': 781,\n",
       " 'transport': 782,\n",
       " 'cells': 783,\n",
       " 'hair': 784,\n",
       " 'bald': 785,\n",
       " 'head': 786,\n",
       " 'shave': 787,\n",
       " 'grew': 788,\n",
       " 'intially': 789,\n",
       " 'loose': 790,\n",
       " 'ideal': 791,\n",
       " 'retirement': 792,\n",
       " 'stance': 793,\n",
       " 'possession': 794,\n",
       " 'weapons': 795,\n",
       " 'mass': 796,\n",
       " 'destruction': 797,\n",
       " 'north': 798,\n",
       " 'korea': 799,\n",
       " 'websites': 800,\n",
       " 'escorts': 801,\n",
       " 'escort': 802,\n",
       " 'polo': 803,\n",
       " 'diesel': 804,\n",
       " 'grand': 805,\n",
       " 'petrol': 806,\n",
       " 'diet': 807,\n",
       " 'growing': 808,\n",
       " 'decathlete': 809,\n",
       " 'black': 810,\n",
       " 'hole': 811,\n",
       " 'finite': 812,\n",
       " 'morgan': 813,\n",
       " 'freeman': 814,\n",
       " 'correct': 815,\n",
       " 'says': 816,\n",
       " 'racism': 817,\n",
       " 'talking': 818,\n",
       " 'quote': 819,\n",
       " 'fab': 820,\n",
       " 'currently': 821,\n",
       " 'offer': 822,\n",
       " 'stock': 823,\n",
       " 'options': 824,\n",
       " 'uber': 825,\n",
       " 'reviews': 826,\n",
       " 'distribute': 827,\n",
       " 'identical': 828,\n",
       " 'pencils': 829,\n",
       " 'least': 830,\n",
       " 'pencil': 831,\n",
       " 'apples': 832,\n",
       " 'distributed': 833,\n",
       " 'among': 834,\n",
       " 'children': 835,\n",
       " 'hire': 836,\n",
       " 'jerry': 837,\n",
       " 'hours': 838,\n",
       " 'tape': 839,\n",
       " 'hour': 840,\n",
       " 'presentation': 841,\n",
       " '7': 842,\n",
       " 'days': 843,\n",
       " 'late': 844,\n",
       " 'rabies': 845,\n",
       " 'vaccine': 846,\n",
       " 'non': 847,\n",
       " 'bite': 848,\n",
       " 'exposure': 849,\n",
       " 'injection': 850,\n",
       " 'dog': 851,\n",
       " 'britain': 852,\n",
       " 'ruled': 853,\n",
       " 'standard': 854,\n",
       " 'amount': 855,\n",
       " 'given': 856,\n",
       " 'relocation': 857,\n",
       " 'afraid': 858,\n",
       " 'working': 859,\n",
       " 'everything': 860,\n",
       " 'red': 861,\n",
       " 'keys': 862,\n",
       " 'season': 863,\n",
       " 'orange': 864,\n",
       " 'lose': 865,\n",
       " 'virginity': 866,\n",
       " 'forgetful': 867,\n",
       " 'forget': 868,\n",
       " 'wife': 869,\n",
       " 'killed': 870,\n",
       " 'emperor': 871,\n",
       " 'monetize': 872,\n",
       " 'videos': 873,\n",
       " 'upload': 874,\n",
       " 'copyright': 875,\n",
       " 'chances': 876,\n",
       " 'may': 877,\n",
       " 'issue': 878,\n",
       " 'switch': 879,\n",
       " 'canon': 880,\n",
       " '6d': 881,\n",
       " 'lux': 882,\n",
       " 'tripod': 883,\n",
       " 'members': 884,\n",
       " 'moderation': 885,\n",
       " 'moderator': 886,\n",
       " 'nobody': 887,\n",
       " 'answer': 888,\n",
       " 'answering': 889,\n",
       " 'funniest': 890,\n",
       " 'joke': 891,\n",
       " 'got': 892,\n",
       " 'essex': 893,\n",
       " 'london': 894,\n",
       " 'pm': 895,\n",
       " 'deduction': 896,\n",
       " 'pls': 897,\n",
       " 'advice': 898,\n",
       " 'tentative': 899,\n",
       " 'monthly': 900,\n",
       " 'expenses': 901,\n",
       " 'saving': 902,\n",
       " 'leaving': 903,\n",
       " 'norway': 904,\n",
       " 'mortgage': 905,\n",
       " 'bsu': 906,\n",
       " 'sell': 907,\n",
       " 'apartment': 908,\n",
       " 'stereotypes': 909,\n",
       " 'kingdom': 910,\n",
       " 'americans': 911,\n",
       " 'twitter': 912,\n",
       " 'business': 913,\n",
       " 'source': 914,\n",
       " 'transfer': 915,\n",
       " 'paypal': 916,\n",
       " 'send': 917,\n",
       " 'withdraw': 918,\n",
       " 'earphone': 919,\n",
       " 'deep': 920,\n",
       " 'bass': 921,\n",
       " 'earphones': 922,\n",
       " 'eligible': 923,\n",
       " 'yojana': 924,\n",
       " 'employee': 925,\n",
       " 'iec': 926,\n",
       " 'forward': 927,\n",
       " 'engineering': 928,\n",
       " 'fields': 929,\n",
       " 'suit': 930,\n",
       " 'abusing': 931,\n",
       " 'abusive': 932,\n",
       " 'several': 933,\n",
       " 'ex': 934,\n",
       " 'past': 935,\n",
       " 'coworkers': 936,\n",
       " 'bosses': 937,\n",
       " 'abusers': 938,\n",
       " 'attract': 939,\n",
       " 'gross': 940,\n",
       " 'ctc': 941,\n",
       " 'seems': 942,\n",
       " 'weekly': 943,\n",
       " 'paycheck': 944,\n",
       " 'yearly': 945,\n",
       " 'employer': 946,\n",
       " 'agreed': 947,\n",
       " 'upon': 948,\n",
       " 'prospering': 949,\n",
       " 'towns': 950,\n",
       " 'kerala': 951,\n",
       " 'confidence': 952,\n",
       " 'clarity': 953,\n",
       " 'anyone': 954,\n",
       " 'relation': 955,\n",
       " 'greek': 956,\n",
       " 'gods': 957,\n",
       " 'hindu': 958,\n",
       " 'parallelism': 959,\n",
       " 'draw': 960,\n",
       " 'mythology': 961,\n",
       " 'egyptian': 962,\n",
       " 'apple': 963,\n",
       " 'music': 964,\n",
       " 'fixed': 965,\n",
       " 'fund': 966,\n",
       " 'live': 967,\n",
       " 'cologne': 968,\n",
       " 'k': 969,\n",
       " 'ln': 970,\n",
       " 'robert': 971,\n",
       " 'de': 972,\n",
       " 'al': 973,\n",
       " 'regard': 974,\n",
       " 'actor': 975,\n",
       " 'prefer': 976,\n",
       " 'small': 977,\n",
       " 'families': 978,\n",
       " 'animals': 979,\n",
       " 'kiss': 980,\n",
       " 'besides': 981,\n",
       " 'deleted': 982,\n",
       " 'chats': 983,\n",
       " 'view': 984,\n",
       " 'dms': 985,\n",
       " 'addicted': 986,\n",
       " 'hired': 987,\n",
       " 'private': 988,\n",
       " 'eyes': 989,\n",
       " 'ordered': 990,\n",
       " 'follow': 991,\n",
       " 'able': 992,\n",
       " 'investigators': 993,\n",
       " 'startup': 994,\n",
       " 'accelerator': 995,\n",
       " 'accelerators': 996,\n",
       " 'funding': 997,\n",
       " 'check': 998,\n",
       " 'wifi': 999,\n",
       " 'history': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "embeddings=np.random.randn(len(vocabulary)+1, embedding_dim)\n",
    "embeddings[0]=0\n",
    "\n",
    "# build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    embeddings[index]=word2vec.word_vec(word)\n",
    "    \n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length= max(train_df.question1.map(lambda x: len(x)).max(),\n",
    "                    train_df.question2.map(lambda x: len(x)).max(),\n",
    "                    test_df.question1.map(lambda x: len(x)).max(),\n",
    "                    test_df.question2.map(lambda x: len(x)).max(),)\n",
    "\n",
    "x=train_df[questions_cols]\n",
    "y=train_df['is_duplicate']\n",
    "X_train, X_validation, Y_train, Y_validation= train_test_split(x, y, test_size=0.2) \n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "X_test = {'left': test_df.question1, 'right': test_df.question2}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "    \n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model variables\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 25\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# Pack it all up into a model\n",
    "malstm = Model([left_input, right_input], [malstm_distance])\n",
    "\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "training_start_time = time()\n",
    "\n",
    "malstm_trained = malstm.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
    "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(malstm_trained.history['acc'])\n",
    "plt.plot(malstm_trained.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(malstm_trained.history['loss'])\n",
    "plt.plot(malstm_trained.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
